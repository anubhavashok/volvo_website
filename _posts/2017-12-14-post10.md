---
layout: post
title: Network to Network Compression using Policy Gradient Reinforcement Learning 
tags:
- Compression 
- Post
---


# Motivation
To be able to develop smart perception systems, we would like to leverage the power of Deep Neural Networks. However, in order to acheive this, we need to account for two critical factors. 1. Performance 2. Speed and 3. Size. Deploying an unreliable model with bad performance might result in orders of magnitude greater harm than good in the real world. Thus, it is imperative that we have models that are robust to varying conditions. Speed is critical since we want to be able to make decisions in real-time - possibly faster than human perception, if we want to prevent accidents. Size is important since the network would be running on an on-board embedded system. While the current Deep Learning community has achieved state of the art results on large datasets with high compute GPU clusters, deploying a high performance model in the real world comes with additional challenges. Table 1 shows the magnitude of time/speed gained by using a smaller network in a practical setting.

|             | CPU Train | CPU Test | TitanX Train | TitanX Test | Memory  |
|-------------|-----------|----------|--------------|-------------|---------|
| ResNet-200  | 22.4 s    | 8.6 s    | 300 ms       | 104 ms      | 5052 MB |
| ResNet-18   | 2.2 s     | 0.8 s    | 31 ms        | 10 ms       | 612 MB  |
| Improvement | 10x       | 10x      | 10x          | 10x         | 8x      |

Table 1

# Overview
In this project, we propose a Model Compression technique that aims to address the above problems. Model Compression algorithms take as input a large model which performs well on some task and generate a smaller model with comparable performance. This is beneficial to our problem since a simpler  model overfits less to the training data and produces more robust performance in varying conditions (Parsimony, Occam's razor). Additionally, a smaller neural network is also faster during test time since the forward pass has to perform less convolution operations. Lastly, the reduced size of the network allows us to actually deploy the model in practice.

The Model Compression method we propose, builds on an existing technique, Knowledge Distillation by Hinton et. al 2015. Knowledge Distillation is a Model Compression technique that trains a smaller neural network (student) to mimic the outputs of the larger network (teacher). This results in a compressed model that has comparable performance to the teacher model. Classically, the student model is defined by hand, borrowing motifs from previous hand-designed networks. Our approach innovates on this by introducing a novel architecture search technique that selects an optimal student model using reinforcement learning. We briefly describe the method and results below. We also compare our method to existing compression techniques and show a substantial improvement. Future directions are also discussed. A more detailed discussion of the work in paper form and code can be found at TODO:.

<div class="divider"></div>

# Approach
<img src="https://github.com/anubhavashok/volvo_website/raw/gh-pages/static/img/n2n_pipeline.png" align="middle" style="width: 300px;"/>

At a high level, we train two policies to compress the network in two stages. First, we train a layer removal policy which removes layers from the teacher model. Second, we train a layer shrinkage policy to shrink each layer by reducing the number of parameters in each layer of the best model from the first stage. The result of this two stage is a compressed student model that can be trained using Knowledge Distillation. 

## Reward
Our reward signal has to encode both compression and accuracy. Thus we propose the reward function defined below, which takes both quantities into account and normalizes them with respect to the teacher model and dataset. The non-linearity introduced aids helps to bias the policies towards producing networks that preserve performance on the dataset.
<img src="https://github.com/anubhavashok/volvo_website/raw/gh-pages/static/img/reward.png" />
## Constrained reward
We introduce the concept of a constrained reward. If we know the hardware or software constraints prior to using the algorithm, we can use them to ensure that the algorithm produces models that respect those constraints. 
<img src="https://github.com/anubhavashok/volvo_website/raw/gh-pages/static/img/const.png" />

## Optimization
Our objective function is the expected reward over the entire rollout.
<img src="https://github.com/anubhavashok/volvo_website/raw/gh-pages/static/img/eqn1.png" />

To train both policies we use the REINFORCE algorithm to determine a gradient for the above objective with the reward defined in the previous section. To stabilize training, we average the gradient over a batch size of m (usually 5) and subtract a baseline (moving average of 5 previous rewards). 
<img src="https://github.com/anubhavashok/volvo_website/raw/gh-pages/static/img/eqn2.png" />
<img src="https://github.com/anubhavashok/volvo_website/raw/gh-pages/static/img/eqn3.png" />

## Knowledge Distillation
To determine the accuracy of the student model on the dataset, we first perform 5 rounds of Knowledge Distillation and then test on the validation set.
The objective function for Knowledge Distillation simply minimizes L-2 distance between the un-normalized logits of the teacher and student.
<img src="https://github.com/anubhavashok/volvo_website/raw/gh-pages/static/img/eqn4.png" />

<div class="divider"></div>

# Results
In this section we present results for our method on a variety of different settings. We also compare our method to existing compression baselines and find that it outperforms them.
## Compression experiments
<img src="https://github.com/anubhavashok/volvo_website/raw/gh-pages/static/img/table1.png" />
To determine the effectiveness of the method, we tested it on a variety of datasets of varying difficulty such as MNIST, CIFAR-10/100, SVHN, Caltech256 and ImageNet32. Details about the experimental procedure are given in the paper
<img src="https://github.com/anubhavashok/volvo_website/raw/gh-pages/static/img/graph1.png" />
<img src="https://github.com/anubhavashok/volvo_website/raw/gh-pages/static/img/graph2.png" />
<img src="https://github.com/anubhavashok/volvo_website/raw/gh-pages/static/img/graph3.png" />
## Baselines
We tested the performance of a Pruning method and Knowledge Distillation with a hand-designed model. To ensure fairness, we used the same teacher model for all baselines.
<img src="https://github.com/anubhavashok/volvo_website/raw/gh-pages/static/img/baseline.png" />
<img src="https://github.com/anubhavashok/volvo_website/raw/gh-pages/static/img/baseline2.png" />

<div class="divider"></div>

## Size constraint
Here we apply a constraint that we would likely come across in the real world, a size constraint, and evaluate how well the approach performs.
<img src="https://github.com/anubhavashok/volvo_website/raw/gh-pages/static/img/size_const.png" />


## Transfer learning
We show that the policies generalize across networks in the same family. Notably, we find that pretraining on a smaller teacher network (e.g. ResNet-18) shows a rapid increase in convergence when using the policy on a larger teacher network (e.g. ResNet-34) 
<img src="https://github.com/anubhavashok/volvo_website/raw/gh-pages/static/img/transfer.png" />

## SSD
In addition to classification tasks, we also applied this approach to an object detection task using the SSD model.

<div class="divider"></div>

# Future directions
<div class="divider"></div>

# References 

